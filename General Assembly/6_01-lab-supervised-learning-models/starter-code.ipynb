{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:31.179991Z",
     "start_time": "2022-08-19T01:08:31.169021Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR, SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, \\\n",
    "BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:31.211905Z",
     "start_time": "2022-08-19T01:08:31.180989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('401ksubs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The other 2 variables that would be helpful are length of time in the workforce and white collar worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model trained on the data may carry harmful notions about race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e401k can be dropped as the information is reflected in p401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "incsq and agesq are the variables.\n",
    "<br>\n",
    "<br>\n",
    "These may have been done to check eligibility to participate in 401(k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:31.227863Z",
     "start_time": "2022-08-19T01:08:31.212904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0      0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1      1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2      0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3      0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4      0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "\n",
       "   agesq  \n",
       "0   1600  \n",
       "1   1225  \n",
       "2   1936  \n",
       "3   1936  \n",
       "4   2809  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:31.243820Z",
     "start_time": "2022-08-19T01:08:31.229858Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['e401k', 'inc', 'p401k', 'pira', 'incsq', 'agesq'], axis=1)\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.236743Z",
     "start_time": "2022-08-19T01:08:31.244818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When i=1, Train RMSE=20.70743636701588, Test RMSE=20.348354228903798, Total RMSE=41.055790595919675\n",
      "When i=2, Train RMSE=19.65751780815017, Test RMSE=20.056173167023896, Total RMSE=39.71369097517407\n",
      "When i=3, Train RMSE=19.48301757922989, Test RMSE=19.990244621396865, Total RMSE=39.473262200626756\n",
      "When i=4, Train RMSE=19.114196309535863, Test RMSE=20.05801804749897, Total RMSE=39.17221435703483\n",
      "When i=5, Train RMSE=18.935686589696033, Test RMSE=23.034564225817263, Total RMSE=41.970250815513296\n",
      "When i=6, Train RMSE=18.601040776375417, Test RMSE=121.01761412484716, Total RMSE=139.61865490122258\n",
      "When i=7, Train RMSE=18.3288922739184, Test RMSE=528.0698700865937, Total RMSE=546.3987623605121\n",
      "When i=8, Train RMSE=17.97205539605005, Test RMSE=5768.226557844977, Total RMSE=5786.198613241027\n",
      "When i=9, Train RMSE=17.698426578125027, Test RMSE=38266.83726218625, Total RMSE=38284.53568876438\n",
      "When i=10, Train RMSE=17.451503881685557, Test RMSE=886597.5437645906, Total RMSE=886614.9952684724\n"
     ]
    }
   ],
   "source": [
    "test_rmse_error = []\n",
    "train_rmse_error = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1, random_state = 42)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=i, include_bias=False)\n",
    "    X_train = poly.fit_transform(X_train)\n",
    "    X_test = poly.fit_transform(X_test)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = lr.predict(X_train)\n",
    "    train_rmse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "    train_rmse_error.append(train_rmse)\n",
    "    \n",
    "    y_test_pred = lr.predict(X_test)\n",
    "    test_rmse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "    test_rmse_error.append(test_rmse)\n",
    "    \n",
    "    total_rmse = train_rmse + test_rmse\n",
    "    \n",
    "    print(f\"When i={i}, Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.348444Z",
     "start_time": "2022-08-19T01:08:43.237741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "plt.plot(np.arange(1,11), test_rmse_error, label='Test RMSE')\n",
    "plt.plot(np.arange(1,11), train_rmse_error, label='Train RMSE')\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.xlim(1,10)\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.364403Z",
     "start_time": "2022-08-19T01:08:43.349442Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['e401k', 'inc', 'p401k', 'pira', 'incsq', 'agesq'], axis=1)\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.396316Z",
     "start_time": "2022-08-19T01:08:43.366398Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.412274Z",
     "start_time": "2022-08-19T01:08:43.397314Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.428231Z",
     "start_time": "2022-08-19T01:08:43.414269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=20.70743636701588, Test RMSE=20.348354228903798, Total RMSE=41.055790595919675\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "    \n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "total_rmse = train_rmse + test_rmse\n",
    "    \n",
    "print(f\"Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.832151Z",
     "start_time": "2022-08-19T01:08:43.429229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When i=1, Train RMSE=3.381058333713307, Test RMSE=27.00290547334037, Total RMSE=30.383963807053675\n",
      "When i=3, Train RMSE=15.1188522432413, Test RMSE=20.36734823723291, Total RMSE=35.48620048047421\n",
      "When i=5, Train RMSE=16.580243635619162, Test RMSE=19.262913349677877, Total RMSE=35.84315698529704\n",
      "When i=7, Train RMSE=17.192451454713833, Test RMSE=18.854277030635526, Total RMSE=36.04672848534936\n",
      "When i=9, Train RMSE=17.57673903634422, Test RMSE=18.583918296568285, Total RMSE=36.16065733291251\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(1,11,2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1, random_state = 42)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=i, n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_train_pred = knn.predict(X_train)\n",
    "    y_test_pred = knn.predict(X_test)\n",
    "    \n",
    "    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "    total_rmse = train_rmse + test_rmse\n",
    "    \n",
    "    print(f\"When i={i}, Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.847110Z",
     "start_time": "2022-08-19T01:08:43.833149Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['e401k', 'inc', 'p401k', 'pira', 'incsq', 'agesq'], axis=1)\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.863068Z",
     "start_time": "2022-08-19T01:08:43.848109Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:43.908946Z",
     "start_time": "2022-08-19T01:08:43.864080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=2.2844231307465623, Test RMSE=28.14737432235294, Total RMSE=30.4317974530995\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state=101)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "total_rmse = train_rmse + test_rmse\n",
    "\n",
    "print(f\"Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.330144Z",
     "start_time": "2022-08-19T01:08:43.909943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=8.935684156561425, Test MSE=21.012898536184338, Total MSE=29.948582692745763\n"
     ]
    }
   ],
   "source": [
    "bagreg = BaggingRegressor(random_state=101, n_jobs=-1)\n",
    "bagreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = bagreg.predict(X_train)\n",
    "y_test_pred = bagreg.predict(X_test)\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "total_rmse = train_rmse + test_rmse\n",
    "\n",
    "print(f\"Train RMSE={train_rmse}, Test MSE={test_rmse}, Total MSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.722096Z",
     "start_time": "2022-08-19T01:08:45.331142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=7.762504284377131, Test RMSE=20.02961261918657, Total RMSE=27.7921169035637\n"
     ]
    }
   ],
   "source": [
    "rfreg = RandomForestRegressor(random_state=101, n_jobs=-1)\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rfreg.predict(X_train)\n",
    "y_test_pred = rfreg.predict(X_test)\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "total_rmse = train_rmse + test_rmse\n",
    "\n",
    "print(f\"Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.846763Z",
     "start_time": "2022-08-19T01:08:45.723094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=21.34539689012653, Test RMSE=21.745933575797558, Total RMSE=43.09133046592409\n"
     ]
    }
   ],
   "source": [
    "adaboostreg = AdaBoostRegressor(random_state=101)\n",
    "adaboostreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = adaboostreg.predict(X_train)\n",
    "y_test_pred = adaboostreg.predict(X_test)\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "total_rmse = train_rmse + test_rmse\n",
    "\n",
    "print(f\"Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.862720Z",
     "start_time": "2022-08-19T01:08:45.847785Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['e401k', 'inc', 'p401k', 'pira', 'incsq', 'agesq'], axis=1)\n",
    "y = df['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.878677Z",
     "start_time": "2022-08-19T01:08:45.864716Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:45.894635Z",
     "start_time": "2022-08-19T01:08:45.880685Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.084104Z",
     "start_time": "2022-08-19T01:08:45.896630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE=22.388214264040467, Test RMSE=21.853488741962675, Total RMSE=44.24170300600314\n"
     ]
    }
   ],
   "source": [
    "svm = SVR()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = adaboostreg.predict(X_train)\n",
    "y_test_pred = adaboostreg.predict(X_test)\n",
    "\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "total_rmse = train_rmse + test_rmse\n",
    "\n",
    "print(f\"Train RMSE={train_rmse}, Test RMSE={test_rmse}, Total RMSE={total_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a method drawing samples from our data, with replacement, in order to estimate the population's parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a inverse tree like structure that splits base on a feature. It constantly splits into the branches.\n",
    "\n",
    "However bagged decision trees is a machine learning alogrithm that combines predictions from multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of bagged decision trees uses all the features to deicide the best split while Random Forest uses a random subset of the features when splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest utilises a subset of the features when splitting. This leads to lower variance and therefore can reduce overfitting relative to bagged decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is overfitting on some Linear Regression models that has been feature engineered with Polynomial Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would choose KNN where the K value is 9. Due to the low RMSE train, test scores and generalization score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would do cross validation and SMOTE to duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e401k is a variable informing us if the individual is eligble for 401(k) while p401k let us know if the individual is enrolled in IRA. Utilising p401k in the model will cause data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, this model is appropriate as we are predicting binary outcomes.\n",
    "<br>\n",
    "<br>\n",
    "KNN, this model is not appropriate as the data set is imbalanced.\n",
    "<br>\n",
    "<br>\n",
    "Naive Bayes, this model assumes all features are independent, however in this scenario that is not the case.\n",
    "<br>\n",
    "<br>\n",
    "Decision Trees, this model is not appropriate as the data set is imbalanced.\n",
    "<br>\n",
    "<br>\n",
    "Random Forest, this model is appropriate as not much data preparation is needed.\n",
    "<br>\n",
    "<br>\n",
    "SVM, this model is not appropriate as the data set is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.100061Z",
     "start_time": "2022-08-19T01:08:49.086099Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(['e401k'], axis=1)\n",
    "y = df['e401k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.115021Z",
     "start_time": "2022-08-19T01:08:49.102068Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size =0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T09:06:12.401008Z",
     "start_time": "2022-08-18T09:06:12.389050Z"
    }
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.191816Z",
     "start_time": "2022-08-19T01:08:49.116019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=0.8834311728764825, Test Accuracy=0.875\n",
      "Train F1=0.8262189676727987, Test F1=0.8116883116883117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lydra\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = lg.score(X_train, y_train)\n",
    "y_test_acc = lg.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = lg.predict(X_train)\n",
    "y_pred_test = lg.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.473066Z",
     "start_time": "2022-08-19T01:08:49.192813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=0.8834311728764825, Test Accuracy=0.875\n",
      "Train F1=0.6356846473029046, Test F1=0.4984984984984985\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = lg.score(X_train, y_train)\n",
    "y_test_acc = lg.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = knn.predict(X_train)\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.520935Z",
     "start_time": "2022-08-19T01:08:49.475070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=1.0, Test Accuracy=0.790948275862069\n",
      "Train F1=1.0, Test F1=0.738544474393531\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = dt.score(X_train, y_train)\n",
    "y_test_acc = dt.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = dt.predict(X_train)\n",
    "y_pred_test = dt.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:49.758300Z",
     "start_time": "2022-08-19T01:08:49.521933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=0.9798730082664431, Test Accuracy=0.8631465517241379\n",
      "Train F1=0.973667711598746, Test F1=0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "bagdetree = BaggingClassifier()\n",
    "bagdetree.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = bagdetree.score(X_train, y_train)\n",
    "y_test_acc = bagdetree.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = bagdetree.predict(X_train)\n",
    "y_pred_test = bagdetree.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:50.761617Z",
     "start_time": "2022-08-19T01:08:49.761293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=1.0, Test Accuracy=0.8685344827586207\n",
      "Train F1=1.0, Test F1=0.805111821086262\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = rf.score(X_train, y_train)\n",
    "y_test_acc = rf.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:51.141601Z",
     "start_time": "2022-08-19T01:08:50.762615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=0.885467832754283, Test Accuracy=0.8760775862068966\n",
      "Train F1=0.829041487839771, Test F1=0.8130081300813008\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = adaboost.score(X_train, y_train)\n",
    "y_test_acc = adaboost.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = adaboost.predict(X_train)\n",
    "y_pred_test = adaboost.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:51.156562Z",
     "start_time": "2022-08-19T01:08:51.142598Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T01:08:56.847340Z",
     "start_time": "2022-08-19T01:08:51.158556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy=0.8852282257098358, Test Accuracy=0.875\n",
      "Train F1=0.8285612025769507, Test F1=0.8104575163398693\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_train_acc = svc.score(X_train, y_train)\n",
    "y_test_acc = svc.score(X_test, y_test)\n",
    "\n",
    "y_pred_train = svc.predict(X_train)\n",
    "y_pred_test = svc.predict(X_test)\n",
    "\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train Accuracy={y_train_acc}, Test Accuracy={y_test_acc}\")\n",
    "print(f\"Train F1={f1_train}, Test F1={f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive is when the model predicts an individual is eligible for 401(k) but in reality he/she is not eligible. \n",
    "<br>\n",
    "<br>\n",
    "False negative is when the model predicts an individual is ineligible for 401(k) but in reality he/she is eligible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, false negative is what we would want to minimize as we lose out on potential customers that are eligible for 401(k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As F1 score is a function of precision and recall. We want to maximise precision, the proportion of postively predicated labels that are actually correct, and recall at the same time. Thus F1 score would be an appropriate metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is overfitting in all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would pick random forest as it has the highest accuracy and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would utilise Gridsearch and Cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression - KNN Regressor\n",
    "<br>\n",
    "<br>\n",
    "Classification - Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
